wav2vec 2.0 的结构分成三大块：

CNN 特征编码器 (Feature Encoder)
输入：原始波形 (16kHz)
输出：低帧率的潜在表示 𝑧𝑡​（每 20ms 一帧）
7 层一维卷积 (Conv1d)，带 LayerNorm 和 GELU 激活。
每层 stride 设置不同，整体下采样率约为 320（16kHz → 50Hz，即 20ms 一帧）。
输出维度：
base 模型：768 维
large 模型：1024 维
作用：代替传统的 MFCC/STFT，把原始波形直接压缩成时序特征。

Transformer 编码器 (Contextual Transformer)
输入：特征编码器输出的序列
输出：带上下文的深层表示 𝑐𝑡​
类似 BERT/Transformer-Encoder 堆叠：
base 模型：12 层，768 维，注意力头数 8，参数约 95M
large 模型：24 层，1024 维，注意力头数 16，参数约 317M
每层包含：
多头自注意力 (Multi-Head Self-Attention)
前馈网络 (FFN)，带 GELU
残差连接 + LayerNorm
作用：建模长时依赖，把局部特征 𝑧𝑡融合成全局表征 𝑐𝑡，捕捉音质/失真等跨时间特征。

量化模块 (Quantization + Contrastive Objective, 训练阶段使用)
用 Gumbel Softmax 把部分表示量化为“代码向量”（离散化）
训练时做对比学习：预测未来帧的正确量化向量，区分干扰负样本

Gumbel Softmax + Codebook：
把 CNN 输出的一部分帧映射到 离散向量（类似词表 embedding）。
这是自监督的关键：让模型预测“未来帧的量化结果”。
对比学习目标 (Contrastive Loss)：
给定上下文向量 𝑐𝑡，预测它未来某时刻的“正确量化向量”，并和一堆负样本对比（InfoNCE loss）。
这样模型学到的特征不仅能重建波形，还捕捉到语义/音质信息。

下采样模块 会把帧特征从 768/1024 维压到 32 维，再给两个任务头。
